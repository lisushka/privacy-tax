# The Privacy Tax - Transcript

Because this is a talk about disability and privacy, it does require some content warnings. We will be discussing the NDIS and sex therapy, the non-consensual use of diagnostic tools, various violations of disabled people's privacy, the mental health impact of surveillance technology and insecure medical devices, and technological abuse of non-speaking people.

Hello everyone, and welcome to Linux Conference Australia 2021, from the comfort of your own homes. First of all, I'd like to acknowledge the Wurundjeri people of the Kulin Nation, on whose land this talk was recorded. This is, and always will be, Aboriginal land. Their sovereignty was never ceded. I would also like to acknowledge the Aboriginal and Torres Strait Islander mentors that I had growing up, who had an incredibly positive impact on me throughout my life.

Usually, when I give talks like this, I would have a standard slide that I would use to introduce myself. But this time, because I'm giving a talk about disability and privacy, I want to do this a little bit differently. I want to do something that generally tends to happen to a lot of disabled people; rather than introducing me to you myself, I'm going to have someone else do it for me. This - is a picture of me that was taken with one of the cows at the Mooving Art installation in Shepparton, in 2018. This - is a still image from when I gave This Talk Has Been Disabled, at Melbourne Ruby earlier this year. And this - is a photograph of me at Flinders Street Station, that was taken by a friend, just after the Melbourne lockdown ended.

What do all of these photos have in common? You might think that the answer to that is that I am in them. But let's not ask me. Let's ask an image recognition AI. I trained this AI on my machine, and then ran these three photographs of me through it. What's the one commonality, that they all point out? The walking stick.

And this really comes back to - a question. What does privacy mean? Most people listening to this talk, particularly if you're non-disabled, will have a fairly good idea of what privacy means to you. However, disabled people often have a different conception of privacy to non-disabled people. And that's by necessity. Because - everyone is interconnected, everyone is interdependent, but the types of help that people with disabilities need tend to be very different to the types of help that people without disabilities need. And what that means is that often, if we want help, we have to give up personal information to get it. That might be because we're interacting with systems that require us to prove that we're worthy of help. It might be because we're interacting with people who - don't believe us, or don't understand - how being disabled affects your perception of the world.

I have, on multiple occasions, gone to hospital, and had co-workers sit next to me, while I was being triaged having medical discussions with staff. And if it surprises you that I might *want* a co-worker there while I'm having those discussions, rather than having those discussions on my own, just consider what might happen if I have to provide that information to a healthcare worker, without anyone there to support me. People with disabilities frequently have to give up information about aspects of their life as private as their sex and sexuality, in order to be able to get appropriate help. Notably, the case of the Australian woman with multiple sclerosis, who fought the NDIS over funding for a sex therapist, so that she would be able to have a fulfilling sex life. And a lot of this comes back to - the idea of surveillance technology. Because the woman with multiple sclerosis who was fighting the NDIS, was fighting the NDIS because she couldn't express her sexuality without that support.

But when that information is stored on you, or when you have visible markers of your disability, there are a whole lot of different ways that that can be used to surveil you. To start with, let's start on a lighter note. Let's talk about browser fingerprinting, which is also known as 'a way to track pretty much any disabled person who uses customised accessibility settings, across the web, in perpetuity'. For those of you who aren't familiar with it, browser fingerprinting is a form of user tracking, that relies on the data that your browser sends about it, and your computer as well - your monitor, your general configuration - to any website that it visits. And as a method of tracking, it relies on everyone having *slightly* different browser settings, so that you can combine that with other information that you gather, to build a profile on people. It can be blocked. There are a lot of add-ons that you can install into your browser, that rather than sending your actual browser fingerprint, will just send randomised data. You can even randomise that data for every webpage that you visit. But the reason that this is important is that disabled people's browser fingerprints are much more likely to be rare or unique. If you think about everyone in a workplace, a lot of those workstation configurations will be fairly standard. However, disabled people, by their nature, will tend to customise their systems in a way that works for them. So that means that your accessibility settings can effectively be used to track you around the Internet. Also, disabled people have a tendency to use older software, and older hardware, than non-disabled people. This is for two reasons. The first one is that - it's more expensive to buy newer hardware, and disabled people generally have less disposable income than non-disabled people. But the second one is, is that software that disabled people might use, to rely on getting around the Internet, might not be available on newer systems. And so you're just stuck with it.

Now browser fingerprinting seems fairly innocuous - and yes, that's something that generally tends to get done to everyone. Another thing though, that tends to get done to the general population, but can be more dangerous for people with disabilities, is - facial recognition, or human recognition, using machine learning models. Many, many people have come to Linux Conference Australia, and have given talks on why image recognition, and facial recognition, is a problem. But the reason why it's specifically a problem for people with disabilities is that the subset of people with visible disabilities is smaller than the general population, and there's a lot of different subsets within that, that cover a lot of different vi- visible disabilities. We know that AI tends to enforce pre-existing biases. We've seen it - with the case where Congressman - former Congressman John Lewis, was misidentified, by an AI, as a criminal. Why do you think that is? Because John Lewis is black, and AI doesn't tend to work very well on black people.

But if you consider the risk that people may be misidentified, that doesn't just hold for one wheelchair user being misidentified as another wheelchair user. If a system already has pre-existing biases built into it about your face and your body, then those biases are only going to be magnified, when you're part of that very small subset of people that uses some form of mobility aid. How do we know that the AI's not going to identify two different people as the same person, just because their wheelchairs look alike?

And then there's the matter of identifying adaptive technology specifically. There are a lot of different screen readers. And as a consequence, we can't easily identify screen reader users on the Internet. Admittedly, different types of screen reader users may slightly change the information that people's browsers send to websites that they visit. However, there's no s- one specific pattern that can be used to identify every single screen reader. So the general consensus is - that identifying screen reader users is pretty well impossible, and probably not very useful even if you did want to use it. Except that there are some website owners who *really* want to serve different content to screen reader users. And this is a question that gets asked time and time again. How can we do this? How can we identify screen reader users? How can we identify this particular group of people? Tracking screen reader users is effectively tracking a disability. And besides that, screen reader users really just want to experience websites in the same way as everyone else.

And there are a couple of important corollaries here. What if a blind screen reader user is working with someone who's sighted? Is it that the sighted person will have to go through the website themselves and describe it? Or will the sighted user have to deal with a degraded experience, because the blind user's screen reader is being detected? Not to mention the fact that not all screen reader users are blind. Dyslexic people, and people with limited written proficiency in particular languages, or poor reading skills otherwise, may use screen readers as an alternative, to having to read language that they find difficult to understand. Should they see the visual parts of websites completely differently just because they have this accessibility need, even though they're sighted?

At least in that case though, we're identifying a piece of adaptive technology, that someone has installed themselves on their machine. Because *this* is where we start to get Kafkaesque. 'Autism and the web: using web-searching ta- tasks to detect autism and improve web accessibility'. What?

'We first examine the way a group of participants with autism, and a control group, process the visual information from Web pages, and provide empirical evidence of different visual searching strategies. We then use these differences in visual attention to train a machine learning classifier, which can successfully use the gaze data to distinguish between the two groups with an accuracy of 0.75. At the end of this paper, we review the way forward to improving web accessibility, and automatic autism detection.' I'm sorry - really?

They call this 'improving web accessibility'. I would like to propose an alternative name for this, and I'm sorry, I can't stop myself from laughing, because as far as I'm concerned, this should be called 'Non-Consensual Machine Learning Diagnostics'. These are people who are genuinely pitching [pause] automatic autism detection as an improvement in Web accessibility. Now, at the moment, the ability to identify autistic users relies on eye-tracking software, and presumably, the participants in this study did opt in. However, the researchers also made the point that - if autistic people's gaze tracks differently around a website, it's very likely as well that the way that they move around that website, whether it by- be by mouse or by keyboard, will be different.

And why would you want to know this? [pause] What are you actually going to do, with the information that a particular browser of your website is most likely autistic? Are you going to serve the website to them differently? And then there's a whole bunch of other ethical considerations. Will people be told that this software is being used on them? We're talking about a 0.75 accuracy here, so it's an accuracy, I believe, of about 75 percent. What about the false positives? Because, with an accuracy of 75 percent, there will be false positives. How will those people feel if they're going to websites that are using these non-consensual diagnostics to work out whether people are autistic, and are being misidentified, and don't know why they're being served different websites? More to the point - what i- what if we're performing non-consensual diagnostics on people who don't know that they're autistic?

But that's not the only thing - [sigh] that people do around autism, that is really scary. One other thing that tends to happen to a lot of autistic people, particularly autistic people with intellectual disabilities or other co-morbidities, is that they may be fitted with physical tracking devices. Now - these are not just used on autistic people. They are used on a lot of vulnerable populations. They're also used on people with intellectual and cognitive disabilities, they're used on people with dementia, and they are used far more on people who live in institutional settings. The thing about these tracking devices is that - they are typically fitted, not by the person being tracked, but by caregivers. Whether those caregivers be family members, or whether those caregivers be - people who are running, say, a group home, or another institutional setting. And the disabled person to whom that tracker is fitted may have no say in how their information is used. That information may be automatically sent to any number of people. Apart from all of the obvious reasons why this is an issue, it's also worth considering the fact that - what you have with these physical tracking devices is a treasure trove of data, on members of vulnerable populations. And if someone were to take that tracking data, and to hack into it, they potentially know a whole lot about the whereabouts about a lot of people, who are not necessarily able to easily advocate for themselves.

And then there's the idea that [pause] 'disabled people don't do that'. A lot of people with disabilities tend to be stereotyped as not living fulfilling lives. I would like to think, in this instance, that - I'm probably the anti-stereotype, because I'm very happy with my life. I think that a lot of other disabled people also are. But there is an expectation, that we are not able to do a lot of things that other people might do. And the point at which this gets scary, is when governments decide to track people who are receiving disability benefits on social media, to ensure that they are disabled enough. What that means, is that photographs of people going out and living their lives can be used to cut off support. This is a bit of an interesting one, because - a lot of those programs that assess disabled people for whether they should receive disability benefits tend to ask people to rate things based on their worst day. But the face that people present on social media, even people with disabilities, tends to be their best face. So you're taking people, who you've assessed entirely based on their impairments, and then looking at their ability to do things, and using that as a way to cut off support. But the point at which that becomes really scary is when that's automated. Because [pause] how can an artificial intelligence, or a machine learning model, tell whether that was a photo of you - currently, or whether it was a photo of you from two years ago before you contracted a severe disease, or before you became immunosuppressed, and had to alter your lifestyle? Disabled people can and should be able to do whatever they want to do with their lives - [pause] and tracking their presence on social media is not an answer to that.

So we've talked about surveillance technology. Now we need to talk about data. [pause] And data security. Because, a lot of data security, even for non-disabled people, becomes a- a question of - who do you trust? And, f- to greater or lesser extents, and for many different reasons, all of us will trust different institutions different amounts. The thing with this, though, is that people with disabilities might have very good reasons for not wanting information about their disability to become public. And that may affect the way that they think about their data, and it may affect the way that they think about data security. Inherently, generally, when your personal medical information is extremely private, the threshold for 'who do you trust?' is going to be a lot lower - or a lot higher, rather.

And that's particularly true when you're talking about private medical information for people with disabilities. Because the thing about that private medical information is that very often, it is taken and it is combined into great big data sets that are then put out into the world, 'anonymised'. However, anonymised data sets are never truly anonymous. If you can combine different sources of data, you can often identify individual people in data sets. This has happened many times. It happened very recently in Australia, where journalists were able to de-identify some of the Myki data that the Victorian government had released, 'anonymously'. It's happened in the past as well, where researchers have taken Australian medical billing data, and have been able to de-anonymise that in a proof of concept. It's also happened with the data of New York taxi drivers, which was de-anonymised to the extent that you could actually find out where taxi drivers lived.

And, it may or may not surprise most of you here to learn that governments are really bad at this. Let's look at a couple of cases in point, that all come from Australia.

First of all, the NDIS. Now, the NDIS, as a system, has a lot of flaws. It has a lot of technological flaws. The NDIS website is very very difficult to navigate around. But there are a couple of things that we need to consider, in terms of the NDIS and data security. The first one is that unless you self-manage your NDIS plan, there is a provider, or a middleman, involved somewhere, that stores some of your personal data. Now, you might have control, or you might have some leverage, over the way that the NDIS handles that data, and whether they store it appropriately. You don't necessarily have that leverage over a third-party provider.

The other thing with the NDIS is that people generally have to provide a lot of personal information to it. And some of that personal information can be around things like medical history, but it can also be around, as we saw in the earlier case, things like sexuality or personal care. Things that most people would generally tend to keep very private. Anecdotally, I've heard reports of the NDIS also doing [pause] some very concerning things to try to safeguard participants. One of those is to require people to present the person who is receiving the NDIS plan, along with a photograph of their ID, on a Zoom planning meeting. This is to prove that the person who's receiving care is actually receiving that care.

The issues with this are two-fold. The first one is that it's really, really easy - well, it's a lot easier than governments think it is - to get around that sort of algorithmic check. If you don't have a human checking, we've already- we've already seen in many cases that algorithms are prone to biases. The algorithms that people use to do those checks, themselves, are also prone to biases. The other issue is that if the government is indeed doing that, that's a massive trove of personal data of vulnerable people. And we don't necessarily know how they're storing it, or what kind of data protection measures they're putting in place.

But the NDIS is not actually the Australian Government's biggest mess-up with regards to personal data. A good candidate for that is My Health Record, which, when it was originally created, was an opt-in system. The Australian Government decided that they wanted more Australian citizens to leverage the advantages of an electronic health record, and so, I believe it was in about 2014 or 2015, they made My Health Record opt-out.

Again, there were a couple of problems with this. The first one was that once your data was in My Health Record, control over which medical professionals could see it was limited. So, you might not be able to stop [pause] someone who is seeing you in Emergency from knowing about your mental health history. And that potentially exacerbates existing- inequalities. The other- problem, and this was the big one as far as data security, was that, unless you opted out of My Health Record during that one initial opt-out period, even if you came back and closed your electronic health record later, the government was going to hold on to all of that data. You would not be able to delete it. If there was a data breach, you would have absolutely no recourse.

Fortunately, [sigh] in this instance, the Australian Government later reversed course on this. One case where the Australian Government *didn't* reverse course [pause] was the 2016 Census. Where the then-Turnbull Government decided that they wanted to run the Census online. The website for the 2016 Census suffered from a 'DDoS attack' on the first day that it was up. A 'DDoS attack' that turned out not to be a DDoS attack, it was just traffic from all of the Australians who wanted to fill in the Census, ah crashing the website, because the website couldn't deal with traffic at that scale.

However, the thing that I want to analyse about the 2016 Census is the way that the Government chose to handle anonymised electronic data. Because Census data, if it's used for demographic information, is supposed to be anonymised. At least, historically, it always has been. But in 2016, the Australian Government decided that they wanted to match that Census data to datasets from health and prescription datasets. That meant, that potentially, they could track individuals' names, addresses, and private demographic information, back to the amount of prescriptions that they were receiving for, say, controlled substances - which might be required to manage a disability.

And the result of that [pause] was that disabled people, who did not want those datasets matched, became scared of filling in the Census. I imagine, although no official statistics have been released, that many of them probably didn't. The problem is that when you engender mistrust in marginalised populations - and it holds true for people with disabilities as well, who do have a right for that prescription information not to be shared with everyone - you generally likely undercount them.

And what that means, when you're dealing with something as large and all-encompassing as the Census, that tends to then drive government decisions, is that [pause] disabled people may not get enough funding for things. We probably don't actually get a full picture of what kinds of disabilities Australians actually have, because people are concerned about that data being traced back to individuals. And that means that funding for things tend to be- tends to be reduced. And it means that we tend to not realise the extent of how many people with disabilities are in our community, which generally results in a sort of 'out of sight, out of mind' phenomenon.

So you think governments are bad - at dealing with private health information, and private medical information, and you'd be right. But private companies aren't really any better. One thing that Big Tech tends to like to do, is to come up with solutions for problems, and then pitch them as also being good for people with disabilities. I had the privilege, earlier - or, at the end of last year, of sharing the stage at Accessibility Manchester [pause] with a gentleman by the name of Ben Mustill-Rose, who works in technology for the BBC. Now, he is a screen reader user, he is blind, and he talked about an anecdote of an experience that he had had in Manchester, being made aware of something in a situation, by a member of the public, that made him feel unsafe in a particular situation, where he then chose to leave that situation, where he was talking about, then, Big Tech pitching these solutions to him, but then constraining what he could do with them based on the terms of service. So, if you're using some sort of adaptive device, and you cannot see, but you're using that adaptive device to describe things to you, and you happen to see something that is against that company's terms of service, well, your access potentially just goes kaputt. And then there's the greater question around how that data is stored and what's being done with it. If Big Tech is pitching a lot of these solutions as being good for people with disabilities, and increasing their access to the world, we do have to ask ourselves - what are we giving up for that?

Now you'd think that when you're dealing with actual health data, private companies, who have a profit motive, would probably do a lot better with it that the Australian Government has. On the contrary. It is not unusual to find [pause] news articles like these, where you are talking about data leaks - of large quantities of medical imaging records - that can potentially be traced back to individual patients. It's fairly well known that storing data in an unsecured S3 bucket on the Internet will generally tend to lead to data theft - ah, it's a fairly common phenomenon, it [laughs] shouldn't be nearly as common as it is, but when you're seeing - mm, people who hold other people's medical imaging data have those same kinds of issues, of storing data unsecured, where people can stumble across it, you really wonder what those data protection laws are doing!

But, surely - medical devices, if you're building the *actual hardware* - surely that would have to be secure, right? Well, [pause] there are actually a few really instructive cases here, and this is what I really want to take a deep dive into. Because, one of the first things that I became interested in, when I started going out and trying to get a better understanding of accessibility, was the case of the MiniMed insulin pumps.

This slide is taken from a talk that I've given previously, This Talk Has Been Disabled, and it's just a picture. Of a MiniMed insulin pump that is used by diabetics who are insulin-dependent to manage their blood glucose levels. The thing about the MiniMed insulin pump is that it had a really critical security flaw, and that security flaw led to a lot of unintended questio- oh, unintended consequences, because diabetics started hunting down these obsolete insulin pumps, to create what is called a 'closed-loop system'. They were using, to do this, a piece of open-source code called OpenAPS. What OpenAPS allowed you to do, was to connect an insulin pump to a constant blood glucose monitor, which generally monitored your levels of glucose in your blood. Once you'd hooked those two up, the insulin pump could deliver insulin, based on the information that it was getting from the constant blood glucose monitor, which would prevent diabetics from ending up with either really high, or really low, blood sugar. The thing about OpenAPS, though, is that it relies on a security flaw in those Medtronic MiniMed insulin pumps, like the one that you saw in the picture. There is no way around it, it is impossible to patch that security flaw, there is very little that you can do abo- about it. What the security flaw allows you to do [pause] is that anyone who can communicate on a particular radio frequency, is able to ta- take remote control of that insulin pump.

And for people who want to use a closed-loop system, and want to gain the benefits of closed-loop systems [pause] for the way that they allow you to manage your health, there is an inherent trade-off here between your safety and your privacy. If you're a public figure, who's got insulin-dependent diabetes, and you want to use this closed-loop system, you have to think very carefully about whether people might actually want to do you harm. As illustrated by the fact that a couple of hackers, who didn't think that the FDA, the insura- or, the ah, medical device regulator in the US, was taking these issues seriously, decided to create a mobile application that could be used to remotely hijack these machines, and either deliver too much insulin or completely withhold it. That might be something that you have to consider.

Then there's the case of the CPAP machines. Now, constant positive air pressure machines are used to treat sleep apnoea. Basically, they consist of [pause] a little box, and ah, nn- a hose, and a mask that you fit to your face, that maintains positive air pressure through your airway, which prevents it from collapsing during the middle of the night, waking you up. CPAP machines have been around for a very long time, but it's only more recently that wireless CPAP machines have hit the market. And what a lot of people with sleep apnoea, who depend on these machines to get a good night's sleep, don't know, is that *that data*, from your CPAP machine, isn't necessarily just going to your doctor.

Sending patient data from CPAP machines directly to device providers *and medical insu- insurers*, is a very common practice in the US, which was exposed by an investigation that was done by the journalistic outlet ProPublica. And what ProPublica discovered is that patients are often not actually aware that this is happening - [pause] until, in one notable case, someone *at the insurance company* sent the person who was using a CPAP machine a printout of all their data, before they had it themselves. So, not only do we have to deal with the usual concerns about hacking, but we also then have to consider the fact that [pause] you *don't know*, once that data's in your insurer's hands, what their data protection standards are, and where it might be going. It may seem more sensible [pause] just to use a regular old CPAP machine that doesn't have the wireless enabled.

There are a couple of other medical devices where you may not have the ability to do this, and for anyone who saw Karen Sandler's keynote here a couple of years ago, erm, about trying to get the source code for her pacemaker, you would know that the wireless feature in pacemakers, which are an implanted device that regulates heart rhythm [pause] has been something of a topic of contention. Because as Karen Sandler discovered, there was *one* device manufacturer - *one whole* device manufacturer - that produced a pacemaker available on the US market where it was possible to disable that wireless function.

A couple of other people have looked at this as well. There is a journalist who himself has a pacemaker implanted, who produced a whole investigative feature on this, looking at the consequences of being, basically, perpetually connected to the Internet, with a device that's implanted in your body. He was uncomfortable hooking up his pacemaker wirelessly to verify that it continued to work, despite the constant urgings of his doctor. At least with a CPAP machine, you can return it and get one that's not wirelessly connected. When it's implanted in your body, that same choice would take surgery.

But there's possibly one frontier [pause] that is even more personal than the idea of having a medical device implanted in your body. I tend to talk a lot about Adaptive and Augmentative Communication, which is a form of communication that is used by non-speaking people who may have cerebral palsy, may be autistic, may have had strokes, may have all kinds of other medical issues that affect their ability to speak verbally. The thing with adaptive communication, though, is that a lot of these devices, a lot of these sort of applications that people use, that is their *entire* communication system if they are non-speaking. And a lot of those applications are closed-source, and a lot of those applications do not publish security information publically. The thing though, that m- that in my mind, makes the adaptive communication case [pause] just that little bit more insidious than the idea of having a wireless device permanently implanted in your body with no choice, is the fact that AAC users' applications often directly send information to the person's caregiver, who may have set up that application - sometimes without their knowledge. If your caregiver is abusive, and you want to have a discussion with people on how to get out, there is no privacy. Their eyes are everywhere.

And this brings me to the core concept that I want to discuss here, which is the idea of the privacy tax. Because that's something that a lot of disabled people pay, in order p- to be able to not have to hand over personal information. And sometimes, that's in money. I am white, I work in technology, and that means- I also have fairly low support needs. That means that I have the ability to pay for things like my walking stick, out of pocket, and not have to hand over personal data to systems like the NDIS. But sometimes that privacy tax also takes the form of a cognitive privacy tax. If you want to find [pause] the one AAC provider, that doesn't hand your data over to caregivers automatically, you might have to spend a *lot* of time trying to find that. And as Karen Sandler discovered, when she was trying to find a pacemaker that- where the wireless function could be disabled, it also takes a lot of effort to make requests to companies to find out what information they actually collect on you, or what they might be able to change, just to make you a little bit safer. And sometimes, if you can't pay the privacy tax, or if the privacy tax is too much, you have to deal with the distress of- the desire to keep personal information private stopping you from getting help that you need. I have experienced this; I know very few people with disabilities who haven't.

And ultimately, the privacy tax always magnifies inequality. For several reasons. The first one is that multiply marginalised people are much less likely to be able to pay the privacy tax in the first place. 45% of First Nations people in Australia are on some form of government support. I'm white, and I work in technology, so I, as a disabled person, have the ability to go out and pay for things that I need out of pocket, but when you're on a government benefit, that becomes much harder to do. You also have to consider the fact that- disabled people, and multiply marginalised people, are more likely to be affected by surveillance technology to begin with. Black disabled people are surveilled in ways that white disabled people very rarely are. And as we've seen from the case of John Lewis, the biases that we have culturally generally tend to be built into AI and machine learning, and very few people have an understanding of how that works. All of that ultimately means [pause] that the privacy tax magnifies the effect of existing inequality.

And then there are the cases where you just have no choice - like Karen Sandler with her pacemaker. It is very difficult, as someone who would have to have a device implanted in your body, to know that you are [pause] going to have to jump over a large number of hurdles, and that someone else has the right to deny you the source code to that device that's in your body. There are other situations, like if you have a CPAP machine, and the only CPAP machine that's available is wireless, where you just have to deal with that. Until very recently, and we'll come back to that one in a minute, if you wanted to set up a closed-loop system for delivery of insulin, you had no choice but to use an insulin pump that was insecure.

And that is where open-source software comes into it. Because OpenAPS [pause] has resulted in a whole lot of changes in this regard. Lana Brindley, in her talk about 'Facebook, Dynamite, Uber, Bombs, and You', talks about the idea that 'open-source bad is better than closed-source bad'. She used the specific example of a radiation machine, that was supposed to treat cancer patients, that ended up killing them due to a bug in the source code. She made the point that if that code had been open-source, someone would have looked at it, seen it, found the issues, and resolved them, before it got to that point. And OpenAPS, and the idea of closed-loop systems, has actually resulted in incremental, but very positive change, from people who manufacture these insulin pumps. There are a number of companies now [pause] who have produced secure insulin pumps, that work, out of the box, with this open-source source code that can be used to set up a closed-loop system. That is a really positive change, that has come out of the companies seeing what people have done, and their willingness to work with the producers of this open-source software. It's also come out of the fact that a lot of people with diabetes, who are insulin-dependent, have really pushed for that, because they want to be able to use secure devices. I believe most of those secure devices are only on the market in Europe, but there are moves now to bring them to the US as well.

NVDA is a really good one. This is one of the really early success stories of open-source software reducing the privacy tax. Because until NVDA, the free and open-source screen reader for Windows, came around, if you were blind, and you needed a screen reader, your options were to acquire a Mac with VoiceOver, or to buy some sort of proprietary screen-reader software, generally JAWS. JAWS licenses are *really* expensive, into the four figures. And that meant that if you were a blind person, who potentially [pause] would have been on the lower socio-economic- lower end of the socio-economic scale, due to you being limited from particular jobs, you had to get governments to pay for that for you. Now, if you're blind, and you want to use a screen reader, or if you have, maybe, a blind kid, and you want to set one up, as long as you have a Windows machine, you can just go and install NVDA. No privacy tax required! Anyone who wants this, regardless of their need for it, is able to get hold of it. And that has opened up screen readers to a whole lot of other people, like people with dyslexia, or people with limited reading proficiency, who may not have been able to get them from government plans.

And even in the world of adaptive and augmentative communication, open-source software has made a lot of progress. There is an OpenAAC initiative, that is focused on the idea of producing open-source communication methods for all kinds of different non-speaking, or semi-speaking, people. You've got CoughDrop, which is an application though, while it's paid, the source code is entirely open. Anyone can review it for bugs, anyone can review it for security issues, and, it places a focus on the user being able to control who they share their information with. You've got adaptive and augmentative communication that has been set up for eye-tracking. Entirely open-source, and can be installed on any Windows hardware. And even better, some of the hardware itself is becoming open-source these days. There's the Open Wheelchair project. Admittedly, this doesn't inherently reduce the privacy tax for people immediately, but what it does do, is it provides a baseline, for anyone who wants to go away, and start developing that hardware themselves, and making changes to it. Because at the moment, customising a wheelchair costs an *enormous* amount of money. You can also look at the open-source ventilators that were produced in response to the COVID-19 pandemic. There are a very large number of them, and [pause] they have been really good, because it's meant that anyone who's got the equipment can go in and set up those ventilators themselves. No need to go and ask governments for the money to do it.

So open-source software as a whole is making changes to the privacy tax. But what can we as individuals do about it? Governments like to talk a lot about the concept of cutting taxes. I want to put a different spin on that, and look at the idea of cutting privacy taxes. And that comes down to a few different points. The first one is the idea of empowering people with disabilities to keep their data private. Unless there's a really good reason for you collecting that, just don't. Consider the impact of hacking on them. If we have to store this data, what might happen if it gets hacked? Can we store it anonymously? Can we store [pause] basically, the data and the information that would link it back to a particular person in different places? Can we overhaul our data security? Are we storing things in [tense] an unsecured S3 bucket? And then think, as well, about what and why you track. Because tracking users of assistive technology is not a fix for bad design. Letting people opt-in to customisations is fine, but, if your website's not usable to at least a basic extent for everyone, that's maybe something you should think about.

So, for questions around data collection: do we need to collect this information? Effy Elden has given a really good talk on [pause] gender data, and false assumptions that developers make about gender, a couple of times, and one of the things that they talk about is the fact that gender is often used as a proxy for other things, and most of the time, you don't actually need to know. It's the same for demographic data around disability. Do you really need to know that your users have a disability? Do you really need to know what kind of disability it is? If you do need to collect personal medical information, is this information that it's more difficult for disabled people to provide, or is it riskier for disabled people to provide it? Like, people who've got psychiatric disabilities might not want to give intimate details of their mental health to people without a very good reason. So can we fix that? Can we collect more generalised data? Can we ask questions that are less intrusive? And then you also have to consider, if the information that we're storing does get leaked, who's going to get hurt the most? Is it going to disproportionately hurt people with disabilities, and why?

And there's always the opportunity to do things around open-source. Because, if you work on adaptive technology, [pause] your code should probably be, at the very least, available to your customers. I would argue that there's probably not a lot of good reasons why your code wouldn't be open-source to everyone, but think seriously about why you wouldn't provide it to people who have those devices implanted in their bodies. And, if you can, you can always donate time or money to open-source projects around disability. Established projects like NVDA can always use volunteers. And then you also have to think about the concept of problems without a solution. Like, we don't have open-source pacemakers. We also don't have [pause] an open-source adaptive and augmentative communication system that works entirely on text input by the user. The vast majority of them, in fact all of them as far as I'm aware, either use eye-tracking, or Picture Exchange Communication System, which relies primarily on pictures.

And advocating for broader change is always very important. You might think that [pause] one voice isn't going to get anywhere, but if enough people think that this should change, and tell people about it, we really can change things. And you can see that from the fact that manufacturers of insulin pumps [pause] are now working with the people who produce open source code for closed-loop systems. And accessibility really should be the default everywhere. It's true for [slowly] Web access, but it's also true for the security and privacy of people's personal information. If you have to store information on people, they should know why. Higher standards of governance are also really important there, you want to make sure that your security around information is good, and sometimes that does actually mean advocating for rules around when and how that private information on people can be collected.

I have referenced several other people here. So I do want to thank Ben Mustill-Rose, Karen Sa- Sandler, Lana Brindley, and Effy Elden, for providing me with the framework to be able to give this talk to you today. We have now reached the end of the talk, but if there's anything that I haven't answered, you can find my contacts up there. Thank you for listening to me today, and I do hope that [pause] I've both taught you some more about disability, and given you a framework to understand the privacy tax, and empower people to reduce it. Thank you.

[Back to The Privacy Tax](./README.md)
